{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 22:51:52 - INFO: Started health steps counter\n",
      "2024-12-10 22:51:52 - INFO: Loading health records ...\n",
      "2024-12-10 22:52:38 - INFO: Corresponding health records loaded and parsed\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import logging\n",
    "import os\n",
    "import hashlib\n",
    "import datetime\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import diffprivlib.tools as dp\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from syftbox.lib import Client, SyftPermission\n",
    "\n",
    "\n",
    "def calculate_file_hash(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Calculate SHA-256 hash of a file\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the file to hash\n",
    "\n",
    "    Returns:\n",
    "        str: Hexadecimal string of the hash\n",
    "    \"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        # Read the file in chunks to handle large files efficiently\n",
    "        for chunk in iter(lambda: f.read(4096), b''):\n",
    "            sha256_hash.update(chunk)\n",
    "\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "\n",
    "def should_run(filepath: str) -> bool:\n",
    "    '''\n",
    "    Check whether current file on filepath hash is the same with the one last recorded\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the file to check\n",
    "\n",
    "    Returns:\n",
    "        bool: True if file has changed or no previous hash exists, False otherwise\n",
    "    '''\n",
    "    hashes_file = f\"./hashes/{API_NAME}_last_run\"\n",
    "\n",
    "    # Calculate current file hash\n",
    "    current_hash = calculate_file_hash(filepath)\n",
    "\n",
    "    # If hashes directory or file doesn't exist, we should run\n",
    "    if not os.path.exists(hashes_file):\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        with open(hashes_file, 'r') as f:\n",
    "            stored_hash = json.load(f).get('hash')\n",
    "\n",
    "        # Return True if hashes are different (file has changed)\n",
    "        return current_hash != stored_hash\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        # If there's any error reading the hash, we should run to be safe\n",
    "        return True\n",
    "\n",
    "\n",
    "def record_filehash(filepath: str) -> None:\n",
    "    '''\n",
    "    Store the current filepath hash \n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the file whose hash should be stored\n",
    "    '''\n",
    "    hashes_file = f\"./hashes/{API_NAME}_last_run\"\n",
    "    current_hash = calculate_file_hash(filepath)\n",
    "\n",
    "    # Create hashes directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(hashes_file), exist_ok=True)\n",
    "\n",
    "    # Store hash in JSON format with timestamp for debugging purposes\n",
    "    hash_data = {\n",
    "        'hash': current_hash,\n",
    "        'timestamp': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    with open(hashes_file, 'w') as f:\n",
    "        json.dump(hash_data, f, indent=2)\n",
    "\n",
    "\n",
    "def validate_config(config):\n",
    "    required_keys = ['filepath', 'parameters']\n",
    "    required_params = ['type', 'epsilon', 'bounds']\n",
    "\n",
    "    if not all(key in config for key in required_keys):\n",
    "        raise ValueError(f\"Missing required config keys: {required_keys}\")\n",
    "    if not all(param in config['parameters'] for param in required_params):\n",
    "        raise ValueError(f\"Missing required parameters: {required_params}\")\n",
    "    if config['parameters']['epsilon'] <= 0:\n",
    "        raise ValueError(\"Epsilon must be positive\")\n",
    "\n",
    "\n",
    "# Following code is from https://github.com/OpenMined/cpu_tracker_member/blob/main/main.py\n",
    "def create_restricted_public_folder(filepath: Path) -> None:\n",
    "    \"\"\"\n",
    "    Create an output folder for Health Steps data within the specified path.\n",
    "\n",
    "    This function creates a directory structure for storing Health Steps data under `api_data`. If the directory\n",
    "    already exists, it will not be recreated. Additionally, default permissions for accessing the created folder are set using the\n",
    "    `SyftPermission` mechanism to allow the data to be read by an aggregator.\n",
    "\n",
    "    Args:\n",
    "        path (Path): The base path where the output folder should be created.\n",
    "\n",
    "    \"\"\"\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "    # Set default permissions for the created folder\n",
    "    permissions = SyftPermission.datasite_default(email=client.email)\n",
    "    permissions.read.append(AGGREGATOR_DATASITE)\n",
    "    permissions.save(filepath)\n",
    "\n",
    "\n",
    "def create_private_folder(filepath: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Create a private folder for Health Steps data within the specified path.\n",
    "\n",
    "    This function creates a directory structure for storing Health Steps data under `private/filepath`.\n",
    "    If the directory already exists, it will not be recreated. Additionally, default permissions for\n",
    "    accessing the created folder are set using the `SyftPermission` mechanism, allowing the data to be\n",
    "    accessible only by the owner's email.\n",
    "\n",
    "    Args:\n",
    "        path (Path): The base path where the output folder should be created.\n",
    "\n",
    "    Returns:\n",
    "        Path: The path to the created directory.\n",
    "    \"\"\"\n",
    "    path: Path = filepath / \"private\" / \"health_steps_counter\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Set default permissions for the created folder\n",
    "    permissions = SyftPermission.datasite_default(email=client.email)\n",
    "    permissions.save(path)\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "# Following code is from https://github.com/OpenMined/cpu_tracker_member/blob/main/main.py\n",
    "def convert_record_to_dict(record):\n",
    "    data = {\n",
    "        'type': record.get('type'),\n",
    "        'source_name': record.get('sourceName'),\n",
    "        'source_version': record.get('sourceVersion'),\n",
    "        'unit': record.get('unit'),\n",
    "        'value': record.get('value'),\n",
    "        'creation_date': record.get('creationDate'),\n",
    "        'start_date': record.get('startDate'),\n",
    "        'end_date': record.get('endDate')\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "def read_apple_health(filepath, type_parameter=None):\n",
    "    logger.info(\"Loading health records ...\")\n",
    "\n",
    "    if filepath.endswith('.zip'):\n",
    "        logger.info(\"Unzipping the file\")\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = f.read()\n",
    "\n",
    "        with zipfile.ZipFile(BytesIO(data)) as zip_ref:\n",
    "            with zip_ref.open('apple_health_export/export.xml') as f:\n",
    "                file_content = f.read()\n",
    "    else:\n",
    "        with open(filepath, 'r') as f:\n",
    "            file_content = f.read()\n",
    "\n",
    "    if not file_content:\n",
    "        logger.error(\"No export file found named\")\n",
    "\n",
    "    soup = BeautifulSoup(file_content, features='xml')\n",
    "    records = soup.find_all(\"Record\")\n",
    "\n",
    "    data_list = []\n",
    "    if type_parameter: \n",
    "        for record in records:\n",
    "            if record.get(\"type\") == type_parameter:\n",
    "                data_list.append(convert_record_to_dict(record))\n",
    "    else:\n",
    "        for record in records:\n",
    "            data_list.append(convert_record_to_dict(record))\n",
    "\n",
    "    logger.info(\"Corresponding health records loaded and parsed\")\n",
    "    # Create the initial dataframe from the XML file, and perform cleansing / preparation\n",
    "\n",
    "    return pd.DataFrame(data_list)\n",
    "\n",
    "\n",
    "def clean_up_df(df):\n",
    "\n",
    "    # Columns that should be converted to float\n",
    "    float_columns = ['value']\n",
    "    # Columns that should be converted to datetime\n",
    "    datetime_columns = ['creation_date', 'start_date', 'end_date']\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df[float_columns] = df[float_columns].apply(\n",
    "        pd.to_numeric, errors='coerce')\n",
    "    df[datetime_columns] = df[datetime_columns].apply(\n",
    "        pd.to_datetime, errors='coerce')\n",
    "\n",
    "    # Use end date as the comparison date\n",
    "    df['date'] = df['end_date'].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Started health steps counter\")\n",
    "\n",
    "try:\n",
    "    with open('config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    validate_config(config)\n",
    "except ValueError as e:\n",
    "    logger.error(str(e))\n",
    "    exit()\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(\n",
    "        \"config.yaml not found. Please create config.yaml (see readme for details)!\")\n",
    "    exit()\n",
    "\n",
    "epsilon = PARAMETERS['epsilon']\n",
    "bounds_config = PARAMETERS['bounds']\n",
    "\n",
    "df = read_apple_health(FILEPATH)\n",
    "# df = clean_up_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreation_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreation_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aidp/lib/python3.10/site-packages/pandas/core/generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   6297\u001b[0m ):\n\u001b[1;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aidp/lib/python3.10/site-packages/pandas/core/accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[0;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aidp/lib/python3.10/site-packages/pandas/core/indexes/accessors.py:643\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, PeriodDtype):\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "df['creation_date'] = df['creation_date'].dt.tz_localize(None)\n",
    "df['start_date'] = df['start_date'].dt.tz_localize(None)\n",
    "df['end_date'] = df['end_date'].dt.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HKQuantityTypeIdentifierForcedVitalCapacity',\n",
       "       'HKQuantityTypeIdentifierForcedExpiratoryVolume1',\n",
       "       'HKQuantityTypeIdentifierBodyMassIndex',\n",
       "       'HKQuantityTypeIdentifierHeight',\n",
       "       'HKQuantityTypeIdentifierBodyMass',\n",
       "       'HKQuantityTypeIdentifierHeartRate',\n",
       "       'HKQuantityTypeIdentifierOxygenSaturation',\n",
       "       'HKQuantityTypeIdentifierRespiratoryRate',\n",
       "       'HKQuantityTypeIdentifierBodyFatPercentage',\n",
       "       'HKQuantityTypeIdentifierLeanBodyMass',\n",
       "       'HKQuantityTypeIdentifierStepCount',\n",
       "       'HKQuantityTypeIdentifierDistanceWalkingRunning',\n",
       "       'HKQuantityTypeIdentifierBasalEnergyBurned',\n",
       "       'HKQuantityTypeIdentifierActiveEnergyBurned',\n",
       "       'HKQuantityTypeIdentifierFlightsClimbed',\n",
       "       'HKQuantityTypeIdentifierAppleExerciseTime',\n",
       "       'HKQuantityTypeIdentifierDistanceCycling',\n",
       "       'HKQuantityTypeIdentifierDistanceSwimming',\n",
       "       'HKQuantityTypeIdentifierSwimmingStrokeCount',\n",
       "       'HKQuantityTypeIdentifierWaistCircumference',\n",
       "       'HKQuantityTypeIdentifierRestingHeartRate',\n",
       "       'HKQuantityTypeIdentifierVO2Max',\n",
       "       'HKQuantityTypeIdentifierWalkingHeartRateAverage',\n",
       "       'HKQuantityTypeIdentifierEnvironmentalAudioExposure',\n",
       "       'HKQuantityTypeIdentifierHeadphoneAudioExposure',\n",
       "       'HKQuantityTypeIdentifierWalkingDoubleSupportPercentage',\n",
       "       'HKQuantityTypeIdentifierSixMinuteWalkTestDistance',\n",
       "       'HKQuantityTypeIdentifierAppleStandTime',\n",
       "       'HKQuantityTypeIdentifierWalkingSpeed',\n",
       "       'HKQuantityTypeIdentifierWalkingStepLength',\n",
       "       'HKQuantityTypeIdentifierWalkingAsymmetryPercentage',\n",
       "       'HKQuantityTypeIdentifierStairAscentSpeed',\n",
       "       'HKQuantityTypeIdentifierStairDescentSpeed',\n",
       "       'HKDataTypeSleepDurationGoal',\n",
       "       'HKQuantityTypeIdentifierAppleWalkingSteadiness',\n",
       "       'HKQuantityTypeIdentifierAppleSleepingWristTemperature',\n",
       "       'HKQuantityTypeIdentifierRunningStrideLength',\n",
       "       'HKQuantityTypeIdentifierRunningVerticalOscillation',\n",
       "       'HKQuantityTypeIdentifierRunningGroundContactTime',\n",
       "       'HKQuantityTypeIdentifierHeartRateRecoveryOneMinute',\n",
       "       'HKQuantityTypeIdentifierRunningPower',\n",
       "       'HKQuantityTypeIdentifierEnvironmentalSoundReduction',\n",
       "       'HKQuantityTypeIdentifierRunningSpeed',\n",
       "       'HKQuantityTypeIdentifierAppleSleepingBreathingDisturbances',\n",
       "       'HKQuantityTypeIdentifierTimeInDaylight',\n",
       "       'HKQuantityTypeIdentifierPhysicalEffort',\n",
       "       'HKCategoryTypeIdentifierSleepAnalysis',\n",
       "       'HKCategoryTypeIdentifierAppleStandHour',\n",
       "       'HKCategoryTypeIdentifierMindfulSession',\n",
       "       'HKCategoryTypeIdentifierHighHeartRateEvent',\n",
       "       'HKCategoryTypeIdentifierAudioExposureEvent',\n",
       "       'HKCategoryTypeIdentifierCoughing',\n",
       "       'HKCategoryTypeIdentifierLowCardioFitnessEvent',\n",
       "       'HKCategoryTypeIdentifierHandwashingEvent',\n",
       "       'HKQuantityTypeIdentifierHeartRateVariabilitySDNN'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.0,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.sum(\n",
    "                    [1,2,3,5,10,22,50],\n",
    "                    epsilon=0.5,\n",
    "                    bounds=(1, 50),\n",
    "                ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tenseal as ts\n",
    "import base64\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_he_context():\n",
    "    # Setup TenSEAL context\n",
    "    context = ts.context(\n",
    "        ts.SCHEME_TYPE.CKKS,\n",
    "        poly_modulus_degree=8192,\n",
    "        coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
    "    )\n",
    "    context.generate_galois_keys()\n",
    "    context.global_scale = 2**40\n",
    "\n",
    "    serialized_context = context.serialize()\n",
    "\n",
    "    with open('context.bin', 'wb') as f:\n",
    "        serialized_context = base64.b64encode(serialized_context)\n",
    "        f.write(serialized_context)\n",
    "        logger.info(type(serialized_context))\n",
    "        # Check if we actually got any data\n",
    "        logger.info(len(serialized_context))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bfvhe_context():\n",
    "    # Setup TenSEAL context\n",
    "    context = ts.context(\n",
    "        ts.SCHEME_TYPE.BFV,\n",
    "        poly_modulus_degree=8192,\n",
    "        plain_modulus=1032193,  # This is specific to BFV\n",
    "        coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
    "    )\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tenseal.enc_context.Context at 0x1071f85b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_bfvhe_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 16:07:10 - INFO: <class 'bytes'>\n",
      "2024-12-11 16:07:10 - INFO: 47305292\n"
     ]
    }
   ],
   "source": [
    "create_he_context()\n",
    "\n",
    "with open('context.bin', 'rb') as f:\n",
    "    serialized_data = bytes(f.read())\n",
    "    a = ts.context_from(base64.b64decode(serialized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tenseal.enc_context.Context at 0x3b06ef670>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.15\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.15'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aidp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
