{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/preserving-data-privacy-in-deep-learning-part-3-ae2103c40c22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms \n",
    "from torchvision.transforms import Compose \n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_wd = False # False: non_iid dataset\n",
    "\n",
    "classes_pc = 2 # Classes per client\n",
    "num_clients = 20\n",
    "num_selected = 6\n",
    "num_rounds = 10\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "baseline_num = 100\n",
    "retrain_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10():\n",
    "\n",
    "    df_train = torchvision.datasets.CIFAR10('../data', train=True, download=True)\n",
    "    df_test = torchvision.datasets.CIFAR10('../data', train=False, download=True)\n",
    "\n",
    "    x_train = df_train.data.transpose((0,3,1,2)) # from (N, H, W, C) to (N, C, H, W)\n",
    "    y_train = np.array(df_train.targets)\n",
    "    x_test = df_test.data.transpose((0,3,1,2)) \n",
    "    y_test = np.array(df_test.targets)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_image_data_stats(data_train, labels_train, data_test, labels_test):\n",
    "  print(\"\\nData: \")\n",
    "  print(\" - Train Set: ({},{}), Range: [{:.3f}, {:.3f}], Labels: {},..,{}\".format(\n",
    "    data_train.shape, labels_train.shape, np.min(data_train), np.max(data_train),\n",
    "      np.min(labels_train), np.max(labels_train)))\n",
    "  print(\" - Test Set: ({},{}), Range: [{:.3f}, {:.3f}], Labels: {},..,{}\".format(\n",
    "    data_test.shape, labels_test.shape, np.min(data_train), np.max(data_train),\n",
    "      np.min(labels_test), np.max(labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clients_rand(train_df, n_clients):\n",
    "    '''\n",
    "    train_df: training data\n",
    "    n_clients: number of clients\n",
    "\n",
    "    Returns:\n",
    "    '''\n",
    "\n",
    "    train_len = train_df.shape[0]\n",
    "\n",
    "    # create random percentages for each client, while the last client gets remainders\n",
    "    weights = np.random.randint(5, 100, size=n_clients-1)\n",
    "    clients_dist = ((weights / weights.sum()) * train_len).astype(int)\n",
    "\n",
    "    # \n",
    "    remainder = train_len - clients_dist.sum()\n",
    "\n",
    "    return list(np.append(clients_dist, remainder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_data_realwd(data, labels, n_clients=100, verbose=True):\n",
    "  '''\n",
    "  Splits (data, labels) among 'n_clients s.t. every client can holds any number of classes which is trying to simulate real world dataset\n",
    "  Input:\n",
    "    data : [n_data x shape]\n",
    "    labels : [n_data (x 1)] from 0 to n_labels(10)\n",
    "    n_clients : number of clients\n",
    "    verbose : True/False => True for printing some info, False otherwise\n",
    "  Output:\n",
    "    clients_split : splitted client data into desired format\n",
    "  '''\n",
    "  def break_into(n,m):\n",
    "    ''' \n",
    "    return m random integers with sum equal to n \n",
    "    '''\n",
    "    to_ret = [1 for i in range(m)]\n",
    "    for i in range(n-m):\n",
    "        ind = random.randint(0,m-1)\n",
    "        to_ret[ind] += 1\n",
    "    return to_ret\n",
    "\n",
    "  #### constants ####\n",
    "  n_classes = len(set(labels))\n",
    "  classes = list(range(n_classes))\n",
    "  np.random.shuffle(classes)\n",
    "  label_indcs  = [list(np.where(labels==class_)[0]) for class_ in classes]\n",
    "  \n",
    "  #### classes for each client ####\n",
    "  tmp = [np.random.randint(1,10) for i in range(n_clients)]\n",
    "  total_partition = sum(tmp)\n",
    "\n",
    "  #### create partition among classes to fulfill criteria for clients ####\n",
    "  class_partition = break_into(total_partition, len(classes))\n",
    "\n",
    "  #### applying greedy approach first come and first serve ####\n",
    "  class_partition = sorted(class_partition,reverse=True)\n",
    "  class_partition_split = {}\n",
    "\n",
    "  #### based on class partition, partitioning the label indexes ###\n",
    "  for ind, class_ in enumerate(classes):\n",
    "      class_partition_split[class_] = [list(i) for i in np.array_split(label_indcs[ind],class_partition[ind])]\n",
    "      \n",
    "#   print([len(class_partition_split[key]) for key in  class_partition_split.keys()])\n",
    "\n",
    "  clients_split = []\n",
    "  count = 0\n",
    "  for i in range(n_clients):\n",
    "    n = tmp[i]\n",
    "    j = 0\n",
    "    indcs = []\n",
    "\n",
    "    while n>0:\n",
    "        class_ = classes[j]\n",
    "        if len(class_partition_split[class_])>0:\n",
    "            indcs.extend(class_partition_split[class_][-1])\n",
    "            count+=len(class_partition_split[class_][-1])\n",
    "            class_partition_split[class_].pop()\n",
    "            n-=1\n",
    "        j+=1\n",
    "\n",
    "    ##### sorting classes based on the number of examples it has #####\n",
    "    classes = sorted(classes,key=lambda x:len(class_partition_split[x]),reverse=True)\n",
    "    if n>0:\n",
    "        raise ValueError(\" Unable to fulfill the criteria \")\n",
    "    clients_split.append([data[indcs], labels[indcs]])\n",
    "#   print(class_partition_split)\n",
    "#   print(\"total example \",count)\n",
    "\n",
    "\n",
    "  def print_split(clients_split): \n",
    "    print(\"Data split:\")\n",
    "    for i, client in enumerate(clients_split):\n",
    "      split = np.sum(client[1].reshape(1,-1)==np.arange(n_labels).reshape(-1,1), axis=1)\n",
    "      print(\" - Client {}: {}\".format(i,split))\n",
    "    print()\n",
    "      \n",
    "    if verbose:\n",
    "      print_split(clients_split)\n",
    "  \n",
    "  # clients_split = np.array(clients_split)\n",
    "  \n",
    "  return clients_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_data(data, labels, n_clients=100, classes_per_client=10, shuffle=True, verbose=True):\n",
    "    '''\n",
    "    Splits (data, labels) among 'n_clients s.t. every client can holds 'classes_per_client' number of classes\n",
    "    Input:\n",
    "      data : [n_data x shape]\n",
    "      labels : [n_data (x 1)] from 0 to n_labels\n",
    "      n_clients : number of clients\n",
    "      classes_per_client : number of classes per client\n",
    "      shuffle : True/False => True for shuffling the dataset, False otherwise\n",
    "      verbose : True/False => True for printing some info, False otherwise\n",
    "    Output:\n",
    "      clients_split : client data into desired format\n",
    "    '''\n",
    "    #### constants ####\n",
    "    n_data = data.shape[0]\n",
    "    n_labels = np.max(labels) + 1\n",
    "\n",
    "    ### client distribution ####\n",
    "    data_per_client = clients_rand(data, n_clients)\n",
    "    data_per_client_per_class = [np.maximum(\n",
    "        1, nd // classes_per_client) for nd in data_per_client]\n",
    "\n",
    "    # sort for labels\n",
    "    data_idcs = [[] for i in range(n_labels)]\n",
    "    for j, label in enumerate(labels):\n",
    "        data_idcs[label] += [j]\n",
    "    if shuffle:\n",
    "        for idcs in data_idcs:\n",
    "            np.random.shuffle(idcs)\n",
    "\n",
    "    # split data among clients\n",
    "    clients_split = []\n",
    "    c = 0\n",
    "    for i in range(n_clients):\n",
    "        client_idcs = []\n",
    "\n",
    "        budget = data_per_client[i]\n",
    "        c = np.random.randint(n_labels)\n",
    "        while budget > 0:\n",
    "            take = min(data_per_client_per_class[i], len(data_idcs[c]), budget)\n",
    "\n",
    "            client_idcs += data_idcs[c][:take]\n",
    "            data_idcs[c] = data_idcs[c][take:]\n",
    "\n",
    "            budget -= take\n",
    "            c = (c + 1) % n_labels\n",
    "\n",
    "        clients_split += [(data[client_idcs], labels[client_idcs])]\n",
    "\n",
    "    def print_split(clients_split):\n",
    "        print(\"Data split:\")\n",
    "        for i, client in enumerate(clients_split):\n",
    "            split = np.sum(client[1].reshape(1, -1) ==\n",
    "                           np.arange(n_labels).reshape(-1, 1), axis=1)\n",
    "            print(\" - Client {}: {}\".format(i, split))\n",
    "        print()\n",
    "\n",
    "        if verbose:\n",
    "            print_split(clients_split)\n",
    "\n",
    "    # clients_split = np.array(clients_split)\n",
    "\n",
    "    return clients_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def shuffle_list(data):\n",
    "#   '''\n",
    "#   This function returns the shuffled data\n",
    "#   '''\n",
    "#   for i in range(len(data)):\n",
    "#     tmp_len= len(data[i][0])\n",
    "#     index = [i for i in range(tmp_len)]\n",
    "#     random.shuffle(index)\n",
    "#     data[i][0],data[i][1] = shuffle_list_data(data[i][0],data[i][1])\n",
    "#   return data\n",
    "\n",
    "def shuffle_list(data):\n",
    "    '''\n",
    "    This function returns the shuffled data\n",
    "    \n",
    "    Parameters:\n",
    "    data: List of [x, y] pairs where x is features and y is labels\n",
    "    \n",
    "    Returns:\n",
    "    List of shuffled [x, y] pairs\n",
    "    '''\n",
    "    shuffled_data = []\n",
    "    for client_data in data:\n",
    "        x, y = client_data[0], client_data[1]\n",
    "        shuffled_x, shuffled_y = shuffle_list_data(x, y)\n",
    "        shuffled_data.append([shuffled_x, shuffled_y])\n",
    "    return shuffled_data\n",
    "\n",
    "def shuffle_list_data(x, y):\n",
    "    '''\n",
    "    This function is a helper function, shuffles an\n",
    "    array while maintaining the mapping between x and y\n",
    "    \n",
    "    Parameters:\n",
    "    x: numpy array of features/data\n",
    "    y: numpy array of labels\n",
    "    \n",
    "    Returns:\n",
    "    Tuple of shuffled (x, y)\n",
    "    '''\n",
    "    inds = list(range(len(x)))\n",
    "    random.shuffle(inds)\n",
    "    return x[inds], y[inds]\n",
    "\n",
    "# def shuffle_list_data(x, y):\n",
    "#   '''\n",
    "#   This function is a helper function, shuffles an\n",
    "#   array while maintaining the mapping between x and y\n",
    "#   '''\n",
    "#   inds = list(range(len(x)))\n",
    "#   random.shuffle(inds)\n",
    "#   return x[inds],y[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inputs, labels, transforms=None):\n",
    "        assert inputs.shape[0] == labels.shape[0], print(inputs.shape[0], labels.shape[0])\n",
    "        self.inputs = torch.Tensor(inputs)\n",
    "        self.labels = torch.Tensor(labels).long()\n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.inputs[index], self.labels[index]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return (img, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "\n",
    "\n",
    "def get_default_data_transforms(train=True):\n",
    "    transforms_train = {\n",
    "        'cifar10': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "    }\n",
    "    transforms_eval = {\n",
    "        'cifar10': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "    }\n",
    "\n",
    "    return (transforms_train['cifar10'], transforms_eval['cifar10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(nclients, batch_size, classes_pc=10, real_wd=False, verbose=True):\n",
    "\n",
    "    x_train, y_train, x_test, y_test = get_cifar10()\n",
    "\n",
    "    if verbose:\n",
    "        print_image_data_stats(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    transforms_train, transforms_eval = get_default_data_transforms()\n",
    "\n",
    "    if real_wd:\n",
    "        split = split_image_data_realwd(\n",
    "            x_train, y_train, n_clients=nclients, verbose=verbose)\n",
    "    else:\n",
    "        split = split_image_data(x_train, y_train, n_clients=nclients,\n",
    "                                 classes_per_client=classes_pc, verbose=verbose)\n",
    "\n",
    "    split_tmp = shuffle_list(split)\n",
    "\n",
    "    client_loaders = [torch.utils.data.DataLoader(CustomImageDataset(x, y, transforms_train),\n",
    "                                                  batch_size=batch_size, shuffle=True) for x, y in split_tmp]\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(CustomImageDataset(\n",
    "        x_test, y_test, transforms_eval), batch_size=100, shuffle=False)\n",
    "\n",
    "    return client_loaders, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        output = F.log_softmax(out, dim=1)\n",
    "        return output\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_data(num):\n",
    "  '''\n",
    "  Returns baseline data loader to be used on retraining on global server\n",
    "  Input:\n",
    "        num : size of baseline data\n",
    "  Output:\n",
    "        loader: baseline data loader\n",
    "  '''\n",
    "  xtrain, ytrain, xtmp,ytmp = get_cifar10()\n",
    "  \n",
    "  x , y = shuffle_list_data(xtrain, ytrain)\n",
    "\n",
    "  x, y = x[:num], y[:num]\n",
    "  transform, _ = get_default_data_transforms(train=True)\n",
    "  loader = torch.utils.data.DataLoader(CustomImageDataset(x, y, transform), batch_size=16, shuffle=True)\n",
    "\n",
    "  return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_update(client_model, optimizer, train_loader, epoch=5):\n",
    "    \"\"\"\n",
    "    This function updates/trains client model on client data\n",
    "    \"\"\"\n",
    "    client_model.train()\n",
    "    for e in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = client_model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_syn(client_model, global_model):\n",
    "  '''\n",
    "  This function synchronizes the client model with global model\n",
    "  '''\n",
    "  client_model.load_state_dict(global_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_aggregate(global_model, client_models,client_lens):\n",
    "    \"\"\"\n",
    "    This function has aggregation method 'wmean'\n",
    "    wmean takes the weighted mean of the weights of models\n",
    "    \"\"\"\n",
    "    total = sum(client_lens)\n",
    "    n = len(client_models)\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float()*(n*client_lens[i]/total) for i in range(len(client_models))], 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(global_model, test_loader):\n",
    "    \"\"\"\n",
    "    This function test the global model on test \n",
    "    data and returns test loss and test accuracy \n",
    "    \"\"\"\n",
    "    global_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = global_model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#### Initializing models and optimizer  ####\n",
    "############################################\n",
    "\n",
    "#### global model ##########\n",
    "global_model =  VGG('VGG19').to(device)\n",
    "\n",
    "############# client models ###############################\n",
    "client_models = [ VGG('VGG19').to(device) for _ in range(num_selected)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global modle \n",
    "\n",
    "###### optimizers ################\n",
    "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "####### baseline data ############\n",
    "loader_fixed = baseline_data(baseline_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_data_loaders(classes_pc=classes_pc, nclients=num_clients,\n",
    "                                             batch_size=batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train = []\n",
    "losses_test = []\n",
    "acc_test = []\n",
    "losses_retrain=[]\n",
    "\n",
    "# Runnining FL\n",
    "for r in range(num_rounds):    #Communication round\n",
    "    # select random clients\n",
    "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "    client_lens = [len(train_loader[idx]) for idx in client_idx]\n",
    "\n",
    "    # client update\n",
    "    loss = 0\n",
    "    for i in tqdm(range(num_selected)):\n",
    "      client_syn(client_models[i], global_model)\n",
    "      loss += client_update(client_models[i], opt[i], train_loader[client_idx[i]], epochs)\n",
    "    losses_train.append(loss)\n",
    "\n",
    "    # server aggregate\n",
    "    #### retraining on the global server\n",
    "    loss_retrain =0\n",
    "    for i in tqdm(range(num_selected)):\n",
    "      loss_retrain+= client_update(client_models[i], opt[i], loader_fixed, epoch=retrain_epochs)\n",
    "    losses_retrain.append(loss_retrain)\n",
    "    \n",
    "    ### Aggregating the models\n",
    "    server_aggregate(global_model, client_models,client_lens)\n",
    "    test_loss, acc = test(global_model, test_loader)\n",
    "    losses_test.append(test_loss)\n",
    "    acc_test.append(acc)\n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss_retrain / num_selected, test_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aidp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
